
# 심볼 기반 미분 그래프

미니플로우에서는 기본적으로 어떤 복잡한 연산을 비교적 간단한 연산들의 연결들로 구성하는 기능을 제공합니다.

예를 들어,

<a href="https://www.codecogs.com/eqnedit.php?latex=E&space;=&space;\text{sigmoid}((y&space;-&space;y^{\prime})^2&space;/&space;2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E&space;=&space;\text{sigmoid}((y&space;-&space;y^{\prime})^2&space;/&space;2)" title="E = \text{sigmoid}((y - y^{\prime})^2 / 2)" /></a>

와 같은 식이 있다고 했을 때,

E는 y에서 y^{\prime} 을 뺀 값을 제곱하고, 그것을 2로 나눈 후 sigmoid함수에 넣은 결과값입니다.

이 때 우리가 사용한 값들은 y와 y^{\prime}들이 있고 사용한 연산들은 빼기, 제곱(혹은 곱하기), 나누기, 시그모이드 4가지가 있습니다.

우리는 각각의 연산과 값들을 그래프의 노드로 생각할 수 있습니다. 이 때, 어떤 노드의 결과값은 다른 노드들의 결과값들을 가지고 특정 연산을 수행한 결과임을 알 수 있습니다.

예를 들어 a = b + c 일 때, (b값 노드), (c값 노드), (+값 노드) 가 존재하며 (+값 노드)는 (b값 노드)와 (c값 노드)를 자식 노드로 가지고 있다고 생각할 수 있습니다.

그리고 a의 값은 (+값 노드)의 결과 값과 같습니다.

## Miniflow implementation

Miniflow에서 각 심볼들은 Node 클래스로 표현됩니다.

```python3
class Node:

    def __init__(self, sess, children, trainable=False):
        self.sess = sess
        self.children = children
        self.parents = []
        self.parentNum = 0
        if not hasattr(self, 'result'):
            self.result = None
        self.gradient = None
        self.numGradient = 0
        self.trainable = trainable
        self.initializer_props = None

        for child in children:
            child.parentNum += 1
            child.parents.append(self)
        
        sess.register_node(self)

        self.shape = self.calc_shape(*[child.shape for child in children])
    def calc_result(self):
        ...
    def calc_gradients(self):
        ...
    def calc_shape(self):
        ...
    def __add__(self, a):
        ...
    def __sub__(self, a):
        ...
```

위 소스코드는 `flow/node/__init__.py`의 일부에서 [Node 클래스](flow/node/__init__.py) 구현의 일부를 가져온 것입니다.

Node는 그 자체로는 쓰일 수가 없으며 이를 상속하는 다른 클래스들이 사용됩니다.

Node를 상속하는 클래스들은 크게 3가지 종류로 나눌 수 있는데, Variable, Placeholder, Operations로 구분할 수 있습니다.

### Variable, Placeholder

Variable과 Placeholder는 자식 노드를 가지지 않고 그저 일정한 값을 반환하는 노드입니다. 대부분 그래프에서 가장 말단의 역할을 담당할 수 있습니다.

이 노드들은 초기화 때에 Initial value를 인자로 받으며, set_result 함수를 통해 값을 변경할 수 있습니다.

Variable과 Placeholder는 하는 역할이 거의 동일하지만, Placeholder는 추후에 나올 Optimizer에 의해 자동으로 값이 바뀌지 않고, Variable은 자동으로 값이 바뀐다는 점에서 차이를 보입니다.

```python3
class Variable(Node):

    def __init__(self, sess, value, **kwargs):
        self.result = np.float32(value)
        super().__init__(sess, [], trainable=True, **kwargs)
    
    def calc_shape(self):
        return self.result.shape
    
    def calc_name(self):
        return 'Var({})'.format(self.result.shape)

class Placeholder(Node):

    def __init__(self, sess, value, name):
        self.result = np.float32(value)
        self.name = name
        sess.register_placeholder(self)
        super().__init__(sess, [])
    
    def calc_shape(self):
        return self.result.shape
```

위 소스코드는 Variable과 Placeholder 구현의 전체 소스코드입니다.

Variable의 경우 __init__함수에서 상위 클래스인 Node의 초기화 함수에 trainable인자를 True로 주는 것을 볼 수 있고,

Placeholder의 경우 꼭 이름을 초기화 당시에 지정해주어야 하는 것을 볼 수 있습니다.

### Operations

연산 노드의 경우 항상 1개 이상의 자식을 가지며, 자식노드의 결과값들에 어떠한 연산을 취한 것을 자신의 결과로 합니다.

```python3
class AddNode(Node):

    def calc_result(self, a, b):
        return a + b

    def calc_gradients(self):
        return [
            array_fit_to_shape(self.gradient, self.children[0].shape),
            array_fit_to_shape(self.gradient, self.children[1].shape)
        ]
    
    def calc_shape(self, a, b):
        return shape_broadcast(a, b)
    
    def calc_name(self, a, b):
        return 'Add({},{})'.format(a, b)

class SigmoidNode(Node):

    def calc_result(self, a):
        return 1.0 / (1 + np.exp(-a))

    def calc_gradients(self):
        return [self.result * (1 - self.result) * self.gradient]
    
    def calc_shape(self, a):
        return a
    
    def calc_name(self, a):
        return 'Sigmoid({})'.format(a)
```

위는 덧셈 연산을 수행하는 기초적인 연산노드와 시그모이드 연산노드의 예제입니다.

연산 노드는 항상 calc_result, calc_gradients, calc_shape, calc_name 구현체를 가져야 합니다.

#### calc_result

자식 노드들의 결과값을 가지고 자신의 결과값을 계산합니다.

자식 노드들의 결과값들은 함수의 인자로 차례로 들어오게 됩니다.

#### calc_gradients

자식 노드들의 결과값들의 미분 값을 계산합니다.

calc_result함수를 f, calc_gradients함수를 f^{\prime}이라고 할 때

<a href="https://www.codecogs.com/eqnedit.php?latex=f^{\prime}&space;=&space;[\frac{\partial}{\partial&space;a}&space;f(a,&space;b,&space;...),&space;\frac{\partial}{\partial&space;b}&space;f(a,&space;b,&space;...),&space;...]&space;*&space;\text{gradient}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f^{\prime}&space;=&space;[\frac{\partial}{\partial&space;a}&space;f(a,&space;b,&space;...),&space;\frac{\partial}{\partial&space;b}&space;f(a,&space;b,&space;...),&space;...]&space;*&space;\text{gradient}" title="f^{\prime} = [\frac{\partial}{\partial a} f(a, b, ...), \frac{\partial}{\partial b} f(a, b, ...), ...] * \text{gradient}" /></a>

를 계산해주면 됩니다.

예제에서의 AddNode의 경우 둘 모두, calc_result를 a, b로 편미분 했을 때 결과가 1이므로, self.gradient를 곱해서 둘 모두에 self.gradient를 반환해주면 됩니다.

단, 대부분 값들이 numpy.array이기 때문에, 덧셈과정에서 [auto-broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)이 적용될 수 있기 때문에 gradient를 자식 노드의 결과값의 모양에 맞게 차원을 줄여서 주어야 합니다. 이 과정은 array_fit_to_shape(array, shape) 함수를 통해 이뤄집니다.

#### calc_shape

calc_result의 결과값 어레이가 가질 모양을 예측해서 계산해주어야 합니다. 예측에 필요한 자식 노드들의 결과값 모양은 차례대로 인자로 전달됩니다.

#### calc_name

Node의 이름이 무엇이 될지 결정해주는 함수가 있어야 합니다.

자식노드들의 이름이 차례로 인자로 들어옵니다.

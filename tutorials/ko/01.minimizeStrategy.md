# How to minimize - gradient descent

어떤 값 y가 아래와 같다고 해보자

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" title="y = f_5(f_4(f_3(f_2(f_1(x)))))" /></a>

그리고

<a href="https://www.codecogs.com/eqnedit.php?latex=x_i&space;=&space;f_i(...f_2(f_1(x)))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i&space;=&space;f_i(...f_2(f_1(x)))" title="x_i = f_i(...f_2(f_1(x)))" /></a>

라고 했을 때

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;x_5" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;x_5" title="y = x_5" /></a>

임이 자명하다.

딥러닝 분야에서는 [GradientDescent](https://en.wikipedia.org/wiki/Gradient_descent) 방법이 이러한 복잡한 y값과 같은 식을 최소화하기 위해 
자주 사용되는데, 방법은 대략 이러하다

## Derivation, Chain rule

y값과 x_i들은 지금 x값에 의해 정해지고 있다.

그러므로 y값을 최소화 하기 위해서는 x값을 조정해야 함이 자명하다.

그렇다면 y값을 최소화하기 위한 x값의 조정방법을 어떻게 구하면 좋을까?

이에 대한 한 해결책을 제공하는 것이 바로 이 GradientDescent인데,

<a href="https://www.codecogs.com/eqnedit.php?latex=x&space;:=&space;x&space;-&space;\frac{\partial&space;y}{\partial&space;x}&space;*&space;\text{learningrate}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x&space;:=&space;x&space;-&space;\frac{\partial&space;y}{\partial&space;x}&space;*&space;\text{learningrate}" title="x := x - \frac{\partial y}{\partial x} * \text{learningrate}" /></a>

위와 같은 업데이트를 x에 반복적으로 행하는 것이 골자이다.

<a href="https://www.codecogs.com/eqnedit.php?latex=\text{gradient}_x&space;=&space;\frac{\partial&space;y}{\partial&space;x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\text{gradient}_x&space;=&space;\frac{\partial&space;y}{\partial&space;x}" title="\text{gradient}_x = \frac{\partial y}{\partial x}" /></a>

아까의 식에서 편미분항만 빼서 x의 Gradient라고 하자.

이 Gradient는 현재 위치에서 x가 y값을 결정하는데 영향을 미친 정도라고 대략적으로 해석할 수 있는데,

이 값이 양수이면 x를 아주 조금 증가시켰을 때, y값도 아주 조금 증가함을 의미하며,

이 값이 음수이면 x를 아주 조금 증가시켰을 때, y값은 아주 조금 감소함을 의미한다.

그렇다면 위의 업데이트 처럼 gradient에 아주 작은 어떤 값 (learning rate)을 곱해서 작게 만든 후에 x값에 빼주면 y값이 감소할 것이라고 기대할 수 있다.

## Chain rule

그렇다면 이 gradient는 어떻게 구할 수 있을까?

아래 식을 보자

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f(g(x))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f(g(x))" title="y = f(g(x))" /></a>

여기서 dy/dx를 구해보자

먼저

<a href="https://www.codecogs.com/eqnedit.php?latex=G&space;=&space;g(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?G&space;=&space;g(x)" title="G = g(x)" /></a>

라고 생각해보자. 그렇다면

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f(G)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f(G)" title="y = f(G)" /></a>

이고

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;y}{\partial&space;G}&space;=&space;f^{\prime}(G)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y}{\partial&space;G}&space;=&space;f^{\prime}(G)" title="\frac{\partial y}{\partial G} = f^{\prime}(G)" /></a>

이다. 이제 dG/dx를 구해보면

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;G}{\partial&space;x}&space;=&space;g^{\prime}(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;G}{\partial&space;x}&space;=&space;g^{\prime}(x)" title="\frac{\partial G}{\partial x} = g^{\prime}(x)" /></a>

인데, 마치 분수의 곱셈의 경우와 비슷한 모양으로

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;y}{\partial&space;x}\&space;=&space;\frac{\partial&space;y}{\partial&space;G}&space;*&space;\frac{\partial&space;G}{\partial&space;x}&space;=&space;f^{\prime}(G)&space;*&space;g^{\prime}(x)&space;=&space;f^{\prime}(g(x))&space;*&space;g^{\prime}(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y}{\partial&space;x}\&space;=&space;\frac{\partial&space;y}{\partial&space;G}&space;*&space;\frac{\partial&space;G}{\partial&space;x}&space;=&space;f^{\prime}(G)&space;*&space;g^{\prime}(x)&space;=&space;f^{\prime}(g(x))&space;*&space;g^{\prime}(x)" title="\frac{\partial y}{\partial x}\ = \frac{\partial y}{\partial G} * \frac{\partial G}{\partial x} = f^{\prime}(G) * g^{\prime}(x) = f^{\prime}(g(x)) * g^{\prime}(x)" /></a>

임이 Chain rule에 의해 성립한다.

그렇다면 다시 첫번째 예제인 아래 등식으로 돌아와서,

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" title="y = f_5(f_4(f_3(f_2(f_1(x)))))" /></a>

에서

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;y}{\partial&space;x}&space;=&space;\frac{\partial&space;x_5}{\partial&space;x_4}&space;*&space;\frac{\partial&space;x_4}{\partial&space;x_3}&space;*&space;...&space;*&space;\frac{\partial&space;x_1}{\partial&space;x}&space;=&space;f_5^{\prime}(x_4)&space;*&space;f_4^{\prime}(x_3)&space;*&space;...&space;*&space;f_1^{\prime}(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y}{\partial&space;x}&space;=&space;\frac{\partial&space;x_5}{\partial&space;x_4}&space;*&space;\frac{\partial&space;x_4}{\partial&space;x_3}&space;*&space;...&space;*&space;\frac{\partial&space;x_1}{\partial&space;x}&space;=&space;f_5^{\prime}(x_4)&space;*&space;f_4^{\prime}(x_3)&space;*&space;...&space;*&space;f_1^{\prime}(x)" title="\frac{\partial y}{\partial x} = \frac{\partial x_5}{\partial x_4} * \frac{\partial x_4}{\partial x_3} * ... * \frac{\partial x_1}{\partial x} = f_5^{\prime}(x_4) * f_4^{\prime}(x_3) * ... * f_1^{\prime}(x)" /></a>

임을 알 수 있다. 그러므로 우리는 각 함수의 미분 함수만 알고있다면 x의 gradient를 구할 수 있다.

## Computational graph expression

좀 더 복잡한 케이스에서의 예제를 보고 Chain rule을 가지고 미분 방법을 살펴보고, 그래프로 식을 표현해보자.

만약

<a href="https://www.codecogs.com/eqnedit.php?latex=A(x)&space;=\frac{1}{1&space;&plus;&space;e^{-x}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?A(x)&space;=\frac{1}{1&space;&plus;&space;e^{-x}}" title="A(x) =\frac{1}{1 + e^{-x}}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=W&space;\in&space;\Re&space;^{n*m},&space;x&space;\in&space;\Re&space;^n,&space;b&space;\in&space;\Re&space;^m,&space;\&space;y&space;=&space;A(W*x&space;&plus;&space;b)\" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W&space;\in&space;\Re&space;^{n*m},&space;x&space;\in&space;\Re&space;^n,&space;b&space;\in&space;\Re&space;^m,&space;\&space;y&space;=&space;A(W*x&space;&plus;&space;b)\" title="W \in \Re ^{n*m}, x \in \Re ^n, b \in \Re ^m, \ y = A(W*x + b)\" /></a>

라고 하자. 참고로 여기서 A가 표현하는 함수는 [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)함수라는 유명한 함수이다.

그리고 우리는 여기서 W, b의 값을 움직여서 y값을 최소화 할 것이다.

W, b와 같이 움직이는 값들을 Variable, x와 같이 움직이지 않을 값들을 Placeholder라고 하자.

여기서 사용하는 행렬의 곱셈이나 덧셈의 경우도 모두 함수로 표현하기 위해 새로운 함수 몇개를 더 정의해서 다시 써보자

<a href="https://www.codecogs.com/eqnedit.php?latex=\text{Add}(a,&space;b)&space;=&space;a&space;&plus;&space;b,&space;\&space;\text{Matmul}(a,&space;b)&space;=&space;a&space;*&space;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\text{Add}(a,&space;b)&space;=&space;a&space;&plus;&space;b,&space;\&space;\text{Matmul}(a,&space;b)&space;=&space;a&space;*&space;b" title="\text{Add}(a, b) = a + b, \ \text{Matmul}(a, b) = a * b" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;A(\text{Add}(\text{Matmul}(W,&space;x),&space;b))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;A(\text{Add}(\text{Matmul}(W,&space;x),&space;b))" title="y = A(\text{Add}(\text{Matmul}(W, x), b))" /></a>

가 된다.

그러면

<img src="data:image/png;base64,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" />

와 같은 그래프로 나타낼 수 있다.

우선 아주 간단히, A의 gradient는 당연히 1일 것인데, dA/dA = 1 이기 때문이다.

<a href="https://www.codecogs.com/eqnedit.php?latex=A^{\prime}&space;=&space;A&space;*&space;(1&space;-&space;A)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?A^{\prime}&space;=&space;A&space;*&space;(1&space;-&space;A)" title="A^{\prime} = A * (1 - A)" /></a>

그리고 A인 sigmoid함수는 위와 같은 성질을 가진다는 것을 미리 알린다.

그렇다면

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;A}{\partial&space;\text{Add}}&space;=&space;A&space;*&space;(1&space;-&space;A)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;A}{\partial&space;\text{Add}}&space;=&space;A&space;*&space;(1&space;-&space;A)" title="\frac{\partial A}{\partial \text{Add}} = A * (1 - A)" /></a>

이고

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;\text{Add}}{\partial&space;\text{Matmul}}&space;=&space;1,&space;\&space;\frac{\partial&space;\text{Add}}{\partial&space;\text{b}}&space;=&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;\text{Add}}{\partial&space;\text{Matmul}}&space;=&space;1,&space;\&space;\frac{\partial&space;\text{Add}}{\partial&space;\text{b}}&space;=&space;1" title="\frac{\partial \text{Add}}{\partial \text{Matmul}} = 1, \ \frac{\partial \text{Add}}{\partial \text{b}} = 1" /></a>

이고

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;\text{Matmul}}{\partial&space;W}&space;=&space;x,&space;\&space;\frac{\partial&space;\text{Matmul}}{\partial&space;x}&space;=&space;W" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;\text{Matmul}}{\partial&space;W}&space;=&space;x,&space;\&space;\frac{\partial&space;\text{Matmul}}{\partial&space;x}&space;=&space;W" title="\frac{\partial \text{Matmul}}{\partial W} = x, \ \frac{\partial \text{Matmul}}{\partial x} = W" /></a>

임을 알 수 있다.

이를 이용해서 Variable인 W, b의 gradient를 구하면

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;A}{\partial&space;W}&space;=&space;\frac{\partial&space;A}{\partial&space;\text{Add}}&space;*&space;\frac{\partial&space;\text{Add}}{\partial&space;\text{Matmul}}&space;*&space;\frac{\partial&space;\text{Matmul}}{\partial&space;W}&space;=&space;(A&space;*&space;(1&space;-&space;A))&space;*&space;1&space;*&space;x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;A}{\partial&space;W}&space;=&space;\frac{\partial&space;A}{\partial&space;\text{Add}}&space;*&space;\frac{\partial&space;\text{Add}}{\partial&space;\text{Matmul}}&space;*&space;\frac{\partial&space;\text{Matmul}}{\partial&space;W}&space;=&space;(A&space;*&space;(1&space;-&space;A))&space;*&space;1&space;*&space;x" title="\frac{\partial A}{\partial W} = \frac{\partial A}{\partial \text{Add}} * \frac{\partial \text{Add}}{\partial \text{Matmul}} * \frac{\partial \text{Matmul}}{\partial W} = (A * (1 - A)) * 1 * x" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;A}{\partial&space;b}&space;=&space;\frac{\partial&space;A}{\partial&space;\text{Add}}&space;*&space;\frac{\partial&space;\text{Add}}{\partial&space;b}&space;=&space;(A&space;*&space;(1&space;-&space;A))&space;*&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;A}{\partial&space;b}&space;=&space;\frac{\partial&space;A}{\partial&space;\text{Add}}&space;*&space;\frac{\partial&space;\text{Add}}{\partial&space;b}&space;=&space;(A&space;*&space;(1&space;-&space;A))&space;*&space;1" title="\frac{\partial A}{\partial b} = \frac{\partial A}{\partial \text{Add}} * \frac{\partial \text{Add}}{\partial b} = (A * (1 - A)) * 1" /></a>

임을 알 수 있다. (단 위 식에서, 좌우변에서 행렬의 모양이 맞지 않거나 곱셈이 되지 않는 경우가 있는데, Matmul의 미분에서 Transpose와 Gradient를 구할 때 곱셈의 순서등을 고려하지 않고 대충 표현했기 떄문이다. 실제로 구현할 때는 특정 순서를 따라서 곱해야 한다.)

즉, 우리는 그래프에서 각 노드들의 계산결과와 Gradient들을 순차적으로 구하면 간단하게 모든 노드들의 Gradient들을 구할 수 있고, Gradient가 계산결과가 전파되는 것과는 반대방향으로 전파되는 식으로 계산되는 것을 알 수 있다.
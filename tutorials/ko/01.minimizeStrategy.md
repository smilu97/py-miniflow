# How to minimize - gradient descent

어떤 값 y가 아래와 같다고 해보자

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" title="y = f_5(f_4(f_3(f_2(f_1(x)))))" /></a>

그리고

<a href="https://www.codecogs.com/eqnedit.php?latex=x_i&space;=&space;f_i(...f_2(f_1(x)))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i&space;=&space;f_i(...f_2(f_1(x)))" title="x_i = f_i(...f_2(f_1(x)))" /></a>

라고 했을 때

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;x_5" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;x_5" title="y = x_5" /></a>

임이 자명하다.

딥러닝 분야에서는 [GradientDescent](https://en.wikipedia.org/wiki/Gradient_descent) 방법이 이러한 복잡한 y값과 같은 식을 최소화하기 위해 
자주 사용되는데, 방법은 대략 이러하다

## Derivation, Chain rule

y값과 x_i들은 지금 x값에 의해 정해지고 있다.

그러므로 y값을 최소화 하기 위해서는 x값을 조정해야 함이 자명하다.

그렇다면 y값을 최소화하기 위한 x값의 조정방법을 어떻게 구하면 좋을까?

이에 대한 한 해결책을 제공하는 것이 바로 이 GradientDescent인데,

<a href="https://www.codecogs.com/eqnedit.php?latex=x&space;:=&space;x&space;-&space;\frac{\partial&space;y}{\partial&space;x}&space;*&space;\text{learningrate}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x&space;:=&space;x&space;-&space;\frac{\partial&space;y}{\partial&space;x}&space;*&space;\text{learningrate}" title="x := x - \frac{\partial y}{\partial x} * \text{learningrate}" /></a>

위와 같은 업데이트를 x에 반복적으로 행하는 것이 골자이다.

<a href="https://www.codecogs.com/eqnedit.php?latex=\text{gradient}_x&space;=&space;\frac{\partial&space;y}{\partial&space;x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\text{gradient}_x&space;=&space;\frac{\partial&space;y}{\partial&space;x}" title="\text{gradient}_x = \frac{\partial y}{\partial x}" /></a>

아까의 식에서 편미분항만 빼서 x의 Gradient라고 하자.

이 Gradient는 현재 위치에서 x가 y값을 결정하는데 영향을 미친 정도라고 대략적으로 해석할 수 있는데,

이 값이 양수이면 x를 아주 조금 증가시켰을 때, y값도 아주 조금 증가함을 의미하며,

이 값이 음수이면 x를 아주 조금 증가시켰을 때, y값은 아주 조금 감소함을 의미한다.

그렇다면 위의 업데이트 처럼 gradient에 아주 작은 어떤 값 (learning rate)을 곱해서 작게 만든 후에 x값에 빼주면 y값이 감소할 것이라고 기대할 수 있다.

## Chain rule

그렇다면 이 gradient는 어떻게 구할 수 있을까?

아래 식을 보자

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f(g(x))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f(g(x))" title="y = f(g(x))" /></a>

여기서 dy/dx를 구해보자

먼저

<a href="https://www.codecogs.com/eqnedit.php?latex=G&space;=&space;g(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?G&space;=&space;g(x)" title="G = g(x)" /></a>

라고 생각해보자. 그렇다면

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f(G)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f(G)" title="y = f(G)" /></a>

이고

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;y}{\partial&space;G}&space;=&space;f^{\prime}(G)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y}{\partial&space;G}&space;=&space;f^{\prime}(G)" title="\frac{\partial y}{\partial G} = f^{\prime}(G)" /></a>

이다. 이제 dG/dx를 구해보면

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;G}{\partial&space;x}&space;=&space;g^{\prime}(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;G}{\partial&space;x}&space;=&space;g^{\prime}(x)" title="\frac{\partial G}{\partial x} = g^{\prime}(x)" /></a>

인데, 마치 분수의 곱셈의 경우와 비슷한 모양으로

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;y}{\partial&space;x}\&space;=&space;\frac{\partial&space;y}{\partial&space;G}&space;*&space;\frac{\partial&space;G}{\partial&space;x}&space;=&space;f^{\prime}(G)&space;*&space;g^{\prime}(x)&space;=&space;f^{\prime}(g(x))&space;*&space;g^{\prime}(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y}{\partial&space;x}\&space;=&space;\frac{\partial&space;y}{\partial&space;G}&space;*&space;\frac{\partial&space;G}{\partial&space;x}&space;=&space;f^{\prime}(G)&space;*&space;g^{\prime}(x)&space;=&space;f^{\prime}(g(x))&space;*&space;g^{\prime}(x)" title="\frac{\partial y}{\partial x}\ = \frac{\partial y}{\partial G} * \frac{\partial G}{\partial x} = f^{\prime}(G) * g^{\prime}(x) = f^{\prime}(g(x)) * g^{\prime}(x)" /></a>

임이 Chain rule에 의해 성립한다.

그렇다면 다시 첫번째 예제인 아래 등식으로 돌아와서,

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" title="y = f_5(f_4(f_3(f_2(f_1(x)))))" /></a>

에서

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;y}{\partial&space;x}&space;=&space;\frac{\partial&space;x_5}{\partial&space;x_4}&space;*&space;\frac{\partial&space;x_4}{\partial&space;x_3}&space;*&space;...&space;*&space;\frac{\partial&space;x_1}{\partial&space;x}&space;=&space;f_5^{\prime}(x_4)&space;*&space;f_4^{\prime}(x_3)&space;*&space;...&space;*&space;f_1^{\prime}(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y}{\partial&space;x}&space;=&space;\frac{\partial&space;x_5}{\partial&space;x_4}&space;*&space;\frac{\partial&space;x_4}{\partial&space;x_3}&space;*&space;...&space;*&space;\frac{\partial&space;x_1}{\partial&space;x}&space;=&space;f_5^{\prime}(x_4)&space;*&space;f_4^{\prime}(x_3)&space;*&space;...&space;*&space;f_1^{\prime}(x)" title="\frac{\partial y}{\partial x} = \frac{\partial x_5}{\partial x_4} * \frac{\partial x_4}{\partial x_3} * ... * \frac{\partial x_1}{\partial x} = f_5^{\prime}(x_4) * f_4^{\prime}(x_3) * ... * f_1^{\prime}(x)" /></a>

임을 알 수 있다. 그러므로 우리는 각 함수의 미분 함수만 알고있다면 x의 gradient를 구할 수 있다.
# How to minimize - gradient descent

어떤 값 y가 아래와 같다고 해보자

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" title="y = f_5(f_4(f_3(f_2(f_1(x)))))" /></a>

그리고

<a href="https://www.codecogs.com/eqnedit.php?latex=x_i&space;=&space;f_i(...f_2(f_1(x)))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i&space;=&space;f_i(...f_2(f_1(x)))" title="x_i = f_i(...f_2(f_1(x)))" /></a>

라고 했을 때

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;x_5" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;x_5" title="y = x_5" /></a>

임이 자명하다.

딥러닝 분야에서는 [GradientDescent](https://en.wikipedia.org/wiki/Gradient_descent) 방법이 이러한 복잡한 y값과 같은 식을 최소화하기 위해 
자주 사용되는데, 방법은 대략 이러하다

## Derivation, Chain rule

y값과 x_i들은 지금 x값에 의해 정해지고 있다.

그러므로 y값을 최소화 하기 위해서는 x값을 조정해야 함이 자명하다.

그렇다면 y값을 최소화하기 위한 x값의 조정방법을 어떻게 구하면 좋을까?

이에 대한 한 해결책을 제공하는 것이 바로 이 GradientDescent인데,

<a href="https://www.codecogs.com/eqnedit.php?latex=x&space;:=&space;x&space;-&space;\frac{\partial&space;y}{\partial&space;x}&space;*&space;\text{learningrate}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x&space;:=&space;x&space;-&space;\frac{\partial&space;y}{\partial&space;x}&space;*&space;\text{learningrate}" title="x := x - \frac{\partial y}{\partial x} * \text{learningrate}" /></a>

위와 같은 업데이트를 x에 반복적으로 행하는 것이 골자이다.

<a href="https://www.codecogs.com/eqnedit.php?latex=\text{gradient}_x&space;=&space;\frac{\partial&space;y}{\partial&space;x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\text{gradient}_x&space;=&space;\frac{\partial&space;y}{\partial&space;x}" title="\text{gradient}_x = \frac{\partial y}{\partial x}" /></a>

아까의 식에서 편미분항만 빼서 x의 Gradient라고 하자.

이 Gradient는 현재 위치에서 x가 y값을 결정하는데 영향을 미친 정도라고 대략적으로 해석할 수 있는데,

이 값이 양수이면 x를 아주 조금 증가시켰을 때, y값도 아주 조금 증가함을 의미하며,

이 값이 음수이면 x를 아주 조금 증가시켰을 때, y값은 아주 조금 감소함을 의미한다.

그렇다면 위의 업데이트 처럼 gradient에 아주 작은 어떤 값 (learning rate)을 곱해서 작게 만든 후에 x값에 빼주면 y값이 감소할 것이라고 기대할 수 있다.

## Chain rule

그렇다면 이 gradient는 어떻게 구할 수 있을까?

아래 식을 보자

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f(g(x))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f(g(x))" title="y = f(g(x))" /></a>

여기서 dy/dx를 구해보자

먼저

<a href="https://www.codecogs.com/eqnedit.php?latex=G&space;=&space;g(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?G&space;=&space;g(x)" title="G = g(x)" /></a>

라고 생각해보자. 그렇다면

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f(G)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f(G)" title="y = f(G)" /></a>

이고

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;y}{\partial&space;G}&space;=&space;f^{\prime}(G)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y}{\partial&space;G}&space;=&space;f^{\prime}(G)" title="\frac{\partial y}{\partial G} = f^{\prime}(G)" /></a>

이다. 이제 dG/dx를 구해보면

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;G}{\partial&space;x}&space;=&space;g^{\prime}(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;G}{\partial&space;x}&space;=&space;g^{\prime}(x)" title="\frac{\partial G}{\partial x} = g^{\prime}(x)" /></a>

인데, 마치 분수의 곱셈의 경우와 비슷한 모양으로

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;y}{\partial&space;x}\&space;=&space;\frac{\partial&space;y}{\partial&space;G}&space;*&space;\frac{\partial&space;G}{\partial&space;x}&space;=&space;f^{\prime}(G)&space;*&space;g^{\prime}(x)&space;=&space;f^{\prime}(g(x))&space;*&space;g^{\prime}(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y}{\partial&space;x}\&space;=&space;\frac{\partial&space;y}{\partial&space;G}&space;*&space;\frac{\partial&space;G}{\partial&space;x}&space;=&space;f^{\prime}(G)&space;*&space;g^{\prime}(x)&space;=&space;f^{\prime}(g(x))&space;*&space;g^{\prime}(x)" title="\frac{\partial y}{\partial x}\ = \frac{\partial y}{\partial G} * \frac{\partial G}{\partial x} = f^{\prime}(G) * g^{\prime}(x) = f^{\prime}(g(x)) * g^{\prime}(x)" /></a>

임이 Chain rule에 의해 성립한다.

그렇다면 다시 첫번째 예제인 아래 등식으로 돌아와서,

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f_5(f_4(f_3(f_2(f_1(x)))))" title="y = f_5(f_4(f_3(f_2(f_1(x)))))" /></a>

에서

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;y}{\partial&space;x}&space;=&space;\frac{\partial&space;x_5}{\partial&space;x_4}&space;*&space;\frac{\partial&space;x_4}{\partial&space;x_3}&space;*&space;...&space;*&space;\frac{\partial&space;x_1}{\partial&space;x}&space;=&space;f_5^{\prime}(x_4)&space;*&space;f_4^{\prime}(x_3)&space;*&space;...&space;*&space;f_1^{\prime}(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;y}{\partial&space;x}&space;=&space;\frac{\partial&space;x_5}{\partial&space;x_4}&space;*&space;\frac{\partial&space;x_4}{\partial&space;x_3}&space;*&space;...&space;*&space;\frac{\partial&space;x_1}{\partial&space;x}&space;=&space;f_5^{\prime}(x_4)&space;*&space;f_4^{\prime}(x_3)&space;*&space;...&space;*&space;f_1^{\prime}(x)" title="\frac{\partial y}{\partial x} = \frac{\partial x_5}{\partial x_4} * \frac{\partial x_4}{\partial x_3} * ... * \frac{\partial x_1}{\partial x} = f_5^{\prime}(x_4) * f_4^{\prime}(x_3) * ... * f_1^{\prime}(x)" /></a>

임을 알 수 있다. 그러므로 우리는 각 함수의 미분 함수만 알고있다면 x의 gradient를 구할 수 있다.

## Computational graph expression

좀 더 복잡한 케이스에서의 예제를 보고 Chain rule을 가지고 미분 방법을 살펴보고, 그래프로 식을 표현해보자.

만약

<a href="https://www.codecogs.com/eqnedit.php?latex=A(x)&space;=\frac{1}{1&space;&plus;&space;e^{-x}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?A(x)&space;=\frac{1}{1&space;&plus;&space;e^{-x}}" title="A(x) =\frac{1}{1 + e^{-x}}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=W&space;\in&space;\Re&space;^{n*m},&space;x&space;\in&space;\Re&space;^n,&space;b&space;\in&space;\Re&space;^m,&space;\&space;y&space;=&space;A(W*x&space;&plus;&space;b)\" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W&space;\in&space;\Re&space;^{n*m},&space;x&space;\in&space;\Re&space;^n,&space;b&space;\in&space;\Re&space;^m,&space;\&space;y&space;=&space;A(W*x&space;&plus;&space;b)\" title="W \in \Re ^{n*m}, x \in \Re ^n, b \in \Re ^m, \ y = A(W*x + b)\" /></a>

라고 하자. 참고로 여기서 A가 표현하는 함수는 [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)함수라는 유명한 함수이다.

그리고 우리는 여기서 W, b의 값을 움직여서 y값을 최소화 할 것이다.

W, b와 같이 움직이는 값들을 Variable, x와 같이 움직이지 않을 값들을 Placeholder라고 하자.

여기서 사용하는 행렬의 곱셈이나 덧셈의 경우도 모두 함수로 표현하기 위해 새로운 함수 몇개를 더 정의해서 다시 써보자

<a href="https://www.codecogs.com/eqnedit.php?latex=\text{Add}(a,&space;b)&space;=&space;a&space;&plus;&space;b,&space;\&space;\text{Matmul}(a,&space;b)&space;=&space;a&space;*&space;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\text{Add}(a,&space;b)&space;=&space;a&space;&plus;&space;b,&space;\&space;\text{Matmul}(a,&space;b)&space;=&space;a&space;*&space;b" title="\text{Add}(a, b) = a + b, \ \text{Matmul}(a, b) = a * b" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;A(\text{Add}(\text{Matmul}(W,&space;x),&space;b))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;A(\text{Add}(\text{Matmul}(W,&space;x),&space;b))" title="y = A(\text{Add}(\text{Matmul}(W, x), b))" /></a>

가 된다.

그러면

![graph](/static/simplegraph.png)

와 같은 그래프로 나타낼 수 있다.

우선 아주 간단히, A의 gradient는 당연히 1일 것인데, dA/dA = 1 이기 때문이다.

<a href="https://www.codecogs.com/eqnedit.php?latex=A^{\prime}&space;=&space;A&space;*&space;(1&space;-&space;A)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?A^{\prime}&space;=&space;A&space;*&space;(1&space;-&space;A)" title="A^{\prime} = A * (1 - A)" /></a>

그리고 A인 sigmoid함수는 위와 같은 성질을 가진다는 것을 미리 알린다.

그렇다면

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;A}{\partial&space;\text{Add}}&space;=&space;A&space;*&space;(1&space;-&space;A)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;A}{\partial&space;\text{Add}}&space;=&space;A&space;*&space;(1&space;-&space;A)" title="\frac{\partial A}{\partial \text{Add}} = A * (1 - A)" /></a>

이고

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;\text{Add}}{\partial&space;\text{Matmul}}&space;=&space;1,&space;\&space;\frac{\partial&space;\text{Add}}{\partial&space;\text{b}}&space;=&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;\text{Add}}{\partial&space;\text{Matmul}}&space;=&space;1,&space;\&space;\frac{\partial&space;\text{Add}}{\partial&space;\text{b}}&space;=&space;1" title="\frac{\partial \text{Add}}{\partial \text{Matmul}} = 1, \ \frac{\partial \text{Add}}{\partial \text{b}} = 1" /></a>

이고

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;\text{Matmul}}{\partial&space;W}&space;=&space;x,&space;\&space;\frac{\partial&space;\text{Matmul}}{\partial&space;x}&space;=&space;W" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;\text{Matmul}}{\partial&space;W}&space;=&space;x,&space;\&space;\frac{\partial&space;\text{Matmul}}{\partial&space;x}&space;=&space;W" title="\frac{\partial \text{Matmul}}{\partial W} = x, \ \frac{\partial \text{Matmul}}{\partial x} = W" /></a>

임을 알 수 있다.

이를 이용해서 Variable인 W, b의 gradient를 구하면

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;A}{\partial&space;W}&space;=&space;\frac{\partial&space;A}{\partial&space;\text{Add}}&space;*&space;\frac{\partial&space;\text{Add}}{\partial&space;\text{Matmul}}&space;*&space;\frac{\partial&space;\text{Matmul}}{\partial&space;W}&space;=&space;(A&space;*&space;(1&space;-&space;A))&space;*&space;1&space;*&space;x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;A}{\partial&space;W}&space;=&space;\frac{\partial&space;A}{\partial&space;\text{Add}}&space;*&space;\frac{\partial&space;\text{Add}}{\partial&space;\text{Matmul}}&space;*&space;\frac{\partial&space;\text{Matmul}}{\partial&space;W}&space;=&space;(A&space;*&space;(1&space;-&space;A))&space;*&space;1&space;*&space;x" title="\frac{\partial A}{\partial W} = \frac{\partial A}{\partial \text{Add}} * \frac{\partial \text{Add}}{\partial \text{Matmul}} * \frac{\partial \text{Matmul}}{\partial W} = (A * (1 - A)) * 1 * x" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;A}{\partial&space;b}&space;=&space;\frac{\partial&space;A}{\partial&space;\text{Add}}&space;*&space;\frac{\partial&space;\text{Add}}{\partial&space;b}&space;=&space;(A&space;*&space;(1&space;-&space;A))&space;*&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;A}{\partial&space;b}&space;=&space;\frac{\partial&space;A}{\partial&space;\text{Add}}&space;*&space;\frac{\partial&space;\text{Add}}{\partial&space;b}&space;=&space;(A&space;*&space;(1&space;-&space;A))&space;*&space;1" title="\frac{\partial A}{\partial b} = \frac{\partial A}{\partial \text{Add}} * \frac{\partial \text{Add}}{\partial b} = (A * (1 - A)) * 1" /></a>

임을 알 수 있다. (단 위 식에서, 좌우변에서 행렬의 모양이 맞지 않거나 곱셈이 되지 않는 경우가 있는데, Matmul의 미분에서 Transpose와 Gradient를 구할 때 곱셈의 순서등을 고려하지 않고 대충 표현했기 떄문이다. 실제로 구현할 때는 특정 순서를 따라서 곱해야 한다.)

즉, 우리는 그래프에서 각 노드들의 계산결과와 Gradient들을 순차적으로 구하면 간단하게 모든 노드들의 Gradient들을 구할 수 있고, Gradient가 계산결과가 전파되는 것과는 반대방향으로 전파되는 식으로 계산되는 것을 알 수 있다.